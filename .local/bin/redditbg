#!/usr/bin/python3
import io
import json
import os
import random
import requests
import subprocess
import urllib.parse
import xdg.BaseDirectory

from PIL import Image

subreddits = ['EarthPorn', 'SpacePorn', 'EyeCandy', 'NaturePics']
sub_url = "http://www.reddit.com/r/{}/top.json"
query_params = {'t': 'day', 'limit': 100}
headers = {'User-Agent': 'linux:redditbg v0.3 (by /u/nolemurs)'}


def screen_resolutions():
    xrandr_lines = subprocess.check_output(
        ['xrandr', '-q'], universal_newlines=True
    ).split('\n')
    lines = [line.strip().split()[0] for line in xrandr_lines if '*' in line]
    return [map(int, line.split('x')) for line in lines]


def get_data_dir():
    directory = os.path.join(xdg.BaseDirectory.xdg_data_home, 'redditbg')
    os.makedirs(directory, exist_ok=True)
    return directory


def load_bad_urls():
    cache_file = os.path.join(get_data_dir(), 'bad_urls')
    try:
        with open(cache_file) as f:
            data = json.load(f)
    except IOError:
        data = {}
    return data


def save_bad_urls(data):
    cache_file = os.path.join(get_data_dir(), 'bad_urls')
    with open(cache_file, 'w') as f:
        json.dump(data, f)


def get_urls():
    urls = []
    for sub in subreddits:
        url = sub_url.format(sub)
        response = requests.get(url, params=query_params, headers=headers)
        response.raise_for_status()
        listings = response.json()['data']['children']
        urls.extend([listing['data']['url'] for listing in listings])
    return urls


def preprocess_url(url):
    if urllib.parse.urlparse(url).netloc == 'imgur.com':
        # Fake a .jpg url to get a raw image
        url = url + '.jpg'
    return url


def fetch_images(urls, number=1, image_acceptable=lambda im, ims: True):
    random.shuffle(urls)
    images = []
    bad_urls = load_bad_urls()
    for url in map(preprocess_url, urls):
        if bad_urls.get(url):
            continue
        response = requests.get(url, stream=True, headers=headers)
        response.raise_for_status()
        if response.headers['Content-Type'].startswith('image/'):
            image = Image.open(io.BytesIO(response.content))
            if image_acceptable(image, images):
                images.append(image)
                if len(images) >= number:
                    break
        else:
            bad_urls[url] = True
    save_bad_urls(bad_urls)
    return images


def save_images(images):
    filenames = []
    directory = get_data_dir()
    for index, image in enumerate(images):
        filename = os.path.join(directory, "image-%s.jpg" % index)
        image.save(filename)
        filenames.append(filename)
    return filenames


def set_desktop(filenames):
    subprocess.call(
        sum((['--bg-fill', filename] for filename in filenames), ['feh'])
    )


def fetch_all_dekstop_images():
    resolutions = screen_resolutions()
    screen_count = len(resolutions)

    def image_acceptable(image, already_accepted):
        x, y = image.size
        return x > 1200 and y > 1000

    urls = get_urls()
    images = fetch_images(urls, screen_count, image_acceptable)
    if len(images) == screen_count:
        filenames = save_images(images)
        set_desktop(filenames)


if __name__ == '__main__':
    fetch_all_dekstop_images()
